#!/usr/bin/env python3
"""
å‰ªè¾‘æ ·æœ¬åˆ†æå™¨ â€” ä»å‰ªè¾‘å‰åéŸ³é¢‘å¯¹æ¯”ä¸­æå–ç¼–è¾‘åå¥½

é€šè¿‡å¯¹æ¯”åŸå§‹å½•éŸ³å’Œå·²å‘å¸ƒçš„å‰ªè¾‘ç‰ˆï¼Œè‡ªåŠ¨è¯†åˆ«ç”¨æˆ·çš„ç¼–è¾‘é£æ ¼ï¼š
- å¡«å……è¯åˆ é™¤ç‡
- é™éŸ³æ®µå¤„ç†é˜ˆå€¼
- å†…å®¹å—åˆ å‡æ¿€è¿›åº¦
- é‡å¤/å£è¯¯å¤„ç†æ–¹å¼

ç”¨æ³•:
    python3 analyze_editing_samples.py \\
        --before /path/to/åŸå§‹å½•éŸ³.mp3 \\
        --after /path/to/å‘å¸ƒç‰ˆ.mp3 \\
        --before-transcript /path/to/before_transcript.json \\
        --after-transcript /path/to/after_transcript.json \\
        --output /path/to/learned_patterns.json

å¦‚æœæ²¡æœ‰æä¾› transcriptï¼Œä¼šæç¤ºä½¿ç”¨é˜¿é‡Œäº‘ FunASR å…ˆè½¬å½•ã€‚

ä¾èµ–: pip install librosa numpy
"""

import argparse
import json
import sys
import re
from pathlib import Path
from difflib import SequenceMatcher
from collections import Counter, defaultdict

# --- å¡«å……è¯å’Œè¯­æ°”è¯å®šä¹‰ ---

FILLER_WORDS = {
    'å—¯', 'å•Š', 'å‘ƒ', 'é¢', 'å“¦', 'å™¢',
    'å°±æ˜¯', 'ç„¶å', 'é‚£ä¸ª', 'è¿™ä¸ª', 'æ‰€ä»¥',
    'å¯¹å¯¹å¯¹', 'å¯¹å¯¹', 'æ˜¯æ˜¯æ˜¯',
    'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å—¯å—¯', 'å—¯å—¯å—¯',
    'å•Šå•Š', 'å‘ƒå‘ƒ'
}

STUTTER_PATTERNS = [
    r'(.{1,3})\1{1,}',  # è¿ç»­é‡å¤ï¼Œå¦‚"é‚£é‚£é‚£"
]

# --- æ–‡æœ¬é¢„å¤„ç† ---

def normalize_text(text):
    """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºå¯¹é½"""
    text = text.strip()
    text = re.sub(r'\s+', '', text)  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
    text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', text)  # ç§»é™¤æ ‡ç‚¹
    return text

def extract_sentences_from_transcript(transcript_data):
    """ä»è½¬å½• JSON æå–å¥å­åˆ—è¡¨"""
    sentences = []

    if isinstance(transcript_data, list):
        # subtitles_words.json æ ¼å¼
        current_sentence = []
        current_speaker = None

        for word in transcript_data:
            if word.get('isSpeakerLabel'):
                if current_sentence:
                    text = ''.join(w['text'] for w in current_sentence)
                    sentences.append({
                        'text': text,
                        'speaker': current_speaker,
                        'start': current_sentence[0].get('start', 0),
                        'end': current_sentence[-1].get('end', 0),
                        'words': current_sentence
                    })
                    current_sentence = []
                current_speaker = word.get('speaker', '')
            elif word.get('isGap'):
                # é™éŸ³æ®µï¼Œæ£€æŸ¥æ—¶é•¿
                gap_duration = word.get('end', 0) - word.get('start', 0)
                if gap_duration > 0.5 and current_sentence:
                    text = ''.join(w['text'] for w in current_sentence)
                    sentences.append({
                        'text': text,
                        'speaker': current_speaker,
                        'start': current_sentence[0].get('start', 0),
                        'end': current_sentence[-1].get('end', 0),
                        'words': current_sentence,
                        'gap_after': gap_duration
                    })
                    current_sentence = []
            else:
                current_sentence.append(word)

        if current_sentence:
            text = ''.join(w['text'] for w in current_sentence)
            sentences.append({
                'text': text,
                'speaker': current_speaker,
                'start': current_sentence[0].get('start', 0),
                'end': current_sentence[-1].get('end', 0),
                'words': current_sentence
            })

    elif isinstance(transcript_data, dict):
        # é˜¿é‡Œäº‘åŸå§‹æ ¼å¼
        for transcript in transcript_data.get('transcripts', []):
            for sent in transcript.get('sentences', []):
                sentences.append({
                    'text': sent.get('text', ''),
                    'speaker': str(sent.get('speaker_id', '')),
                    'start': sent.get('begin_time', 0) / 1000,
                    'end': sent.get('end_time', 0) / 1000,
                    'words': sent.get('words', [])
                })

    return sentences


# --- æ–‡æœ¬å¯¹é½å’Œå·®å¼‚åˆ†æ ---

def align_transcripts(before_sentences, after_sentences):
    """å¯¹é½å‰ªè¾‘å‰åçš„è½¬å½•æ–‡æœ¬ï¼Œæ‰¾å‡ºè¢«åˆ é™¤çš„éƒ¨åˆ†"""
    before_text = [normalize_text(s['text']) for s in before_sentences]
    after_text = [normalize_text(s['text']) for s in after_sentences]

    # ä½¿ç”¨ SequenceMatcher æ‰¾åˆ°æœ€é•¿å…¬å…±å­åºåˆ—
    matcher = SequenceMatcher(None, before_text, after_text)

    kept = []      # ä¿ç•™çš„å¥å­ç´¢å¼•ï¼ˆbefore ä¾§ï¼‰
    deleted = []   # è¢«åˆ é™¤çš„å¥å­ç´¢å¼•ï¼ˆbefore ä¾§ï¼‰

    for op, i1, i2, j1, j2 in matcher.get_opcodes():
        if op == 'equal':
            for i in range(i1, i2):
                kept.append(i)
        elif op == 'delete':
            for i in range(i1, i2):
                deleted.append(i)
        elif op == 'replace':
            # replace å¯èƒ½æ˜¯éƒ¨åˆ†åŒ¹é…ï¼Œå°è¯•ç»†ç²’åº¦å¯¹é½
            for i in range(i1, i2):
                # æ£€æŸ¥ before[i] æ˜¯å¦åœ¨ after[j1:j2] ä¸­æœ‰è¿‘ä¼¼åŒ¹é…
                best_ratio = 0
                for j in range(j1, j2):
                    ratio = SequenceMatcher(None, before_text[i], after_text[j]).ratio()
                    best_ratio = max(best_ratio, ratio)
                if best_ratio > 0.7:
                    kept.append(i)
                else:
                    deleted.append(i)

    return kept, deleted


def classify_deletion(sentence, before_sentences, idx, deleted_indices):
    """å¯¹åˆ é™¤çš„å¥å­è¿›è¡Œåˆ†ç±»"""
    text = sentence['text']
    normalized = normalize_text(text)

    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯å¡«å……è¯
    if normalized in FILLER_WORDS or len(normalized) <= 2 and normalized in {'å—¯', 'å•Š', 'å‘ƒ', 'å“¦'}:
        return 'filler_word', text

    # æ£€æŸ¥æ˜¯å¦ä¸ºå¡é¡¿/é‡å¤
    for pattern in STUTTER_PATTERNS:
        if re.search(pattern, normalized):
            return 'stutter', text

    # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­å¥ï¼ˆå¯èƒ½æ˜¯æ®‹å¥ï¼‰
    if len(normalized) < 5:
        return 'residual', text

    # æ£€æŸ¥æ˜¯å¦ä¸ºè¿ç»­åˆ é™¤ï¼ˆå†…å®¹å—ï¼‰
    neighbors_deleted = sum(1 for d in deleted_indices
                            if abs(d - idx) <= 3 and d != idx)
    if neighbors_deleted >= 2:
        return 'content_block', text

    # æ£€æŸ¥é™éŸ³æ®µåçš„åˆ é™¤
    if sentence.get('gap_after', 0) > 2.0:
        return 'silence_related', text

    # é»˜è®¤åˆ†ç±»
    return 'other', text


# --- ç»Ÿè®¡åˆ†æ ---

def analyze_patterns(before_sentences, kept, deleted):
    """ä»åˆ é™¤æ¨¡å¼ä¸­æå–ç»Ÿè®¡è§„å¾‹"""
    total = len(before_sentences)
    deleted_set = set(deleted)

    # ç»Ÿè®¡å„ç±»åˆ é™¤
    deletion_types = defaultdict(list)
    filler_stats = Counter()
    filler_total = Counter()

    for idx in range(total):
        sentence = before_sentences[idx]
        text = normalize_text(sentence['text'])

        # ç»Ÿè®¡å¡«å……è¯æ€»æ•°
        for fw in FILLER_WORDS:
            count = text.count(fw)
            if count > 0:
                filler_total[fw] += count

        if idx in deleted_set:
            dtype, detail = classify_deletion(sentence, before_sentences, idx, deleted)
            deletion_types[dtype].append({
                'idx': idx,
                'text': sentence['text'][:50],
                'duration': sentence.get('end', 0) - sentence.get('start', 0)
            })

            # ç»Ÿè®¡è¢«åˆ é™¤çš„å¡«å……è¯
            for fw in FILLER_WORDS:
                count = text.count(fw)
                if count > 0:
                    filler_stats[fw] += count

    # è®¡ç®—å¡«å……è¯åˆ é™¤ç‡
    filler_deletion_rates = {}
    for fw in filler_total:
        total_count = filler_total[fw]
        deleted_count = filler_stats.get(fw, 0)
        if total_count > 0:
            rate = deleted_count / total_count
            filler_deletion_rates[fw] = {
                'total': total_count,
                'deleted': deleted_count,
                'rate': round(rate, 2)
            }

    # ä¼°è®¡é™éŸ³é˜ˆå€¼
    silence_durations = []
    for idx in deleted_set:
        gap = before_sentences[idx].get('gap_after', 0)
        if gap > 0.5:
            silence_durations.append(gap)

    silence_threshold = None
    if silence_durations:
        silence_threshold = round(min(silence_durations), 1)

    # è®¡ç®—æ—¶é•¿ç»Ÿè®¡
    before_duration = max(s.get('end', 0) for s in before_sentences) if before_sentences else 0
    deleted_duration = sum(
        before_sentences[i].get('end', 0) - before_sentences[i].get('start', 0)
        for i in deleted_set
    )

    return {
        'version': '1.0',
        'summary': {
            'total_sentences': total,
            'kept_sentences': len(kept),
            'deleted_sentences': len(deleted),
            'deletion_rate': round(len(deleted) / total, 2) if total > 0 else 0,
            'before_duration_seconds': round(before_duration, 1),
            'deleted_duration_seconds': round(deleted_duration, 1),
            'reduction_percent': round(deleted_duration / before_duration * 100, 1) if before_duration > 0 else 0
        },
        'deletion_types': {
            dtype: {
                'count': len(items),
                'total_duration': round(sum(i['duration'] for i in items), 1),
                'examples': [i['text'] for i in items[:3]]
            }
            for dtype, items in deletion_types.items()
        },
        'filler_word_analysis': filler_deletion_rates,
        'silence_analysis': {
            'estimated_threshold': silence_threshold,
            'deleted_silence_count': len(silence_durations),
            'silence_durations': sorted(silence_durations)[:10]  # å‰ 10 ä¸ª
        },
        'aggressiveness': classify_aggressiveness(len(deleted) / total if total > 0 else 0),
        'recommendations': generate_recommendations(
            filler_deletion_rates, silence_threshold,
            len(deleted) / total if total > 0 else 0,
            deletion_types
        )
    }


def classify_aggressiveness(deletion_rate):
    """æ ¹æ®åˆ é™¤ç‡åˆ¤æ–­æ¿€è¿›åº¦"""
    if deletion_rate < 0.15:
        return 'conservative'
    elif deletion_rate < 0.30:
        return 'moderate'
    else:
        return 'aggressive'


def generate_recommendations(filler_rates, silence_threshold, deletion_rate, deletion_types):
    """ç”Ÿæˆ editing_rules å»ºè®®"""
    recs = []

    # å¡«å……è¯å»ºè®®
    high_delete = [fw for fw, data in filler_rates.items() if data['rate'] > 0.6]
    low_delete = [fw for fw, data in filler_rates.items() if data['rate'] < 0.3]

    if high_delete:
        recs.append({
            'rule': 'filler_words',
            'action': 'set_high_deletion',
            'words': high_delete,
            'confidence': 0.85,
            'reason': f'è¿™äº›å¡«å……è¯åœ¨æ ·æœ¬ä¸­è¢«åˆ é™¤è¶…è¿‡ 60%: {", ".join(high_delete)}'
        })

    if low_delete:
        recs.append({
            'rule': 'filler_words',
            'action': 'preserve',
            'words': low_delete,
            'confidence': 0.80,
            'reason': f'è¿™äº›å¡«å……è¯åœ¨æ ·æœ¬ä¸­å¤§å¤šè¢«ä¿ç•™: {", ".join(low_delete)}'
        })

    # é™éŸ³é˜ˆå€¼å»ºè®®
    if silence_threshold:
        recs.append({
            'rule': 'silence',
            'action': 'set_threshold',
            'value': silence_threshold,
            'confidence': 0.75,
            'reason': f'æ ·æœ¬ä¸­åˆ é™¤çš„æœ€çŸ­é™éŸ³æ®µä¸º {silence_threshold}s'
        })

    # æ¿€è¿›åº¦å»ºè®®
    recs.append({
        'rule': 'aggressiveness',
        'action': 'set',
        'value': classify_aggressiveness(deletion_rate),
        'confidence': 0.90,
        'reason': f'æ ·æœ¬æ€»ä½“åˆ é™¤ç‡ {deletion_rate:.0%}'
    })

    return recs


# --- ä¸»é€»è¾‘ ---

def main():
    parser = argparse.ArgumentParser(
        description='ä»å‰ªè¾‘å‰åéŸ³é¢‘å¯¹æ¯”ä¸­æå–ç¼–è¾‘åå¥½',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨å·²æœ‰è½¬å½•
  python3 analyze_editing_samples.py \\
      --before-transcript before_words.json \\
      --after-transcript after_words.json \\
      --output learned_patterns.json

  # å®Œæ•´æµç¨‹ï¼ˆéœ€å…ˆç”¨é˜¿é‡Œäº‘è½¬å½•ä¸¤ä¸ªéŸ³é¢‘ï¼‰
  python3 analyze_editing_samples.py \\
      --before åŸå§‹å½•éŸ³.mp3 \\
      --after å‘å¸ƒç‰ˆ.mp3 \\
      --output learned_patterns.json
        """
    )
    parser.add_argument('--before', help='åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--after', help='å‰ªè¾‘åéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--before-transcript', help='åŸå§‹éŸ³é¢‘çš„è½¬å½• JSON')
    parser.add_argument('--after-transcript', help='å‰ªè¾‘åéŸ³é¢‘çš„è½¬å½• JSON')
    parser.add_argument('--output', required=True, help='è¾“å‡º learned_patterns.json è·¯å¾„')

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥
    if not args.before_transcript or not args.after_transcript:
        if args.before and args.after:
            print("âš ï¸  æœªæä¾›è½¬å½•æ–‡ä»¶ã€‚è¯·å…ˆä½¿ç”¨é˜¿é‡Œäº‘ FunASR è½¬å½•ä¸¤ä¸ªéŸ³é¢‘ï¼š", file=sys.stderr)
            print(f"   1. è½¬å½•åŸå§‹éŸ³é¢‘: {args.before}", file=sys.stderr)
            print(f"   2. è½¬å½•å‰ªè¾‘ç‰ˆ: {args.after}", file=sys.stderr)
            print("   3. å°†ä¸¤ä¸ª subtitles_words.json åˆ†åˆ«ä¼ å…¥ --before-transcript å’Œ --after-transcript", file=sys.stderr)
            sys.exit(1)
        else:
            parser.print_help()
            sys.exit(1)

    # åŠ è½½è½¬å½•
    print("ğŸ“– åŠ è½½è½¬å½•æ–‡ä»¶...", file=sys.stderr)
    with open(args.before_transcript, 'r', encoding='utf-8') as f:
        before_data = json.load(f)
    with open(args.after_transcript, 'r', encoding='utf-8') as f:
        after_data = json.load(f)

    # æå–å¥å­
    before_sentences = extract_sentences_from_transcript(before_data)
    after_sentences = extract_sentences_from_transcript(after_data)

    print(f"   åŸå§‹: {len(before_sentences)} å¥", file=sys.stderr)
    print(f"   å‰ªè¾‘ç‰ˆ: {len(after_sentences)} å¥", file=sys.stderr)

    # å¯¹é½å’Œåˆ†æ
    print("ğŸ” å¯¹é½è½¬å½•æ–‡æœ¬...", file=sys.stderr)
    kept, deleted = align_transcripts(before_sentences, after_sentences)
    print(f"   ä¿ç•™: {len(kept)} å¥, åˆ é™¤: {len(deleted)} å¥", file=sys.stderr)

    # åˆ†ææ¨¡å¼
    print("ğŸ“Š åˆ†æç¼–è¾‘æ¨¡å¼...", file=sys.stderr)
    patterns = analyze_patterns(before_sentences, kept, deleted)

    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(patterns, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {output_path}", file=sys.stderr)
    print(f"   æ€»ä½“åˆ å‡ç‡: {patterns['summary']['reduction_percent']}%", file=sys.stderr)
    print(f"   æ¿€è¿›åº¦: {patterns['aggressiveness']}", file=sys.stderr)
    print(f"   å»ºè®®æ•°é‡: {len(patterns['recommendations'])}", file=sys.stderr)

    # è¾“å‡ºåˆ° stdoutï¼ˆä¾›ç®¡é“ä½¿ç”¨ï¼‰
    print(json.dumps(patterns, ensure_ascii=False, indent=2))


if __name__ == '__main__':
    main()
