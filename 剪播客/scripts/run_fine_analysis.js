#!/usr/bin/env node
/**
 * Fine analysis - RULES LAYER ONLY
 * Handles: silence detection, basic stutter detection (consecutive identical words)
 *
 * Semantic analysis (sentence-start fillers, self-correction, in-sentence repeats,
 * residual sentences, repeated sentences) is handled by the LLM layer.
 *
 * Usage: node run_fine_analysis.js [--analysis-dir DIR]
 *
 * Output: fine_analysis_rules.json (merged with LLM output by merge_llm_fine.js)
 */

const fs = require('fs');
const path = require('path');

// Parse args (same convention as merge_llm_fine.js)
let analysisDir = process.cwd();
const dirArgIdx = process.argv.indexOf('--analysis-dir');
if (dirArgIdx >= 0 && process.argv[dirArgIdx + 1]) {
  analysisDir = path.resolve(process.argv[dirArgIdx + 1]);
}

const wordsPath = path.join(analysisDir, '../1_è½¬å½•/subtitles_words.json');
const sentencesPath = path.join(analysisDir, 'sentences.txt');
const analysisPath = path.join(analysisDir, 'semantic_deep_analysis.json');
const outputPath = path.join(analysisDir, 'fine_analysis_rules.json');

const allWords = JSON.parse(fs.readFileSync(wordsPath, 'utf8'));
const sentenceLines = fs.readFileSync(sentencesPath, 'utf8').split('\n').filter(Boolean);
const analysis = JSON.parse(fs.readFileSync(analysisPath, 'utf8'));

// Get deleted sentence indices from 5a
const deletedSentences = new Set(
  analysis.sentences.filter(s => s.action === 'delete').map(s => s.sentenceIdx)
);

const actualWords = allWords.filter(w => !w.isGap && !w.isSpeakerLabel);
const gaps = allWords.filter(w => w.isGap);

// User preferences
const SILENCE_THRESHOLD = 0.8;
const SILENCE_CAP = 0.8;

// === Stutter exemption tiers ===

// Tier 1: å è¯ç™½åå• â€” blanket exempt, never flag
const REDUPLICATED_WORDS = new Set([
  'å¦ˆå¦ˆ', 'çˆ¸çˆ¸', 'å®å®', 'å“¥å“¥', 'å§å§', 'å¼Ÿå¼Ÿ', 'å¥¶å¥¶', 'çˆ·çˆ·',
  'å”å”', 'é˜¿å§¨', 'å©†å©†', 'å…¬å…¬', 'èˆ…èˆ…', 'å§‘å§‘', 'ä¼¯ä¼¯',
  'è°¢è°¢', 'æ˜Ÿæ˜Ÿ', 'å¤šå¤š', 'ç”œç”œ', 'ä¹–ä¹–', 'é¥­é¥­',
  'è¯•è¯•', 'çœ‹çœ‹', 'æƒ³æƒ³', 'è¯´è¯´', 'èŠèŠ', 'èµ°èµ°', 'å¬å¬', 'ç­‰ç­‰',
  'è°ˆè°ˆ', 'è®²è®²', 'å†™å†™', 'è¯»è¯»', 'åå', 'ç©ç©', 'çŒœçŒœ', 'é—®é—®',
  'å“ˆå“ˆ', 'å˜»å˜»', 'å‘µå‘µ', 'å˜¿å˜¿', 'å™—å™—',
]);

// Tier 2: é«˜é¢‘è¯/çŸ­è¯­ â€” NO blanket exemption anymore!
// Rules layer catches them ALL, marks needsReview=true for LLM to decide.
// "æˆ‘æˆ‘è§‰å¾—" â†’ catch + needsReview (LLM å¤§å¤šæ•°ä¼šç¡®è®¤åˆ é™¤)
// "å°±æ˜¯å°±æ˜¯" â†’ catch + needsReview (LLM æ ¹æ®è¯­å¢ƒåˆ¤æ–­)
const MAYBE_NATURAL_REPEATS = new Set([
  'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'å°±', 'å»', 'ä¸', 'ä¹Ÿ', 'éƒ½', 'åœ¨', 'åˆ', 'å¾ˆ', 'å¤ª', 'ä½†', 'è¿˜',
  'æ˜¯', 'æœ‰', 'ä¼š', 'èƒ½', 'è¦', 'æƒ³', 'åš', 'è¯´', 'çœ‹', 'æ¥', 'æ‹‰',
]);
const MAYBE_NATURAL_PHRASES = new Set([
  'å°±æ˜¯', 'æ€ä¹ˆ', 'çœŸçš„æ˜¯', 'çœŸçš„', 'ç„¶å', 'å¯èƒ½', 'å…¶å®', 'åº”è¯¥', 'å·²ç»', 'è¿™æ ·',
]);

// Tier 3: æ•°å­— â€” blanket exempt (e.g. "2022" split into "2","0","2","2")
const NUMBER_CHARS = /^[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿é›¶ä¸¤å‡ å¤šåŠ]+$/;

// English word detection (avoid false positives on "OPEN"+"EN", "THIS"+"IS")
const ENGLISH_WORD = /^[A-Za-z]+$/;

// Parse sentences
const sentences = sentenceLines.map(line => {
  const parts = line.split('|');
  const [startIdx, endIdx] = parts[1].split('-').map(Number);
  return {
    idx: parseInt(parts[0]),
    wordRange: [startIdx, endIdx],
    speaker: parts[2],
    text: parts[3],
    words: actualWords.slice(startIdx, endIdx + 1),
    startTime: actualWords[startIdx] ? actualWords[startIdx].start : 0,
    endTime: actualWords[endIdx] ? actualWords[endIdx].end : 0,
  };
});

const edits = [];
let editIdx = 0;

function getNextSentenceStart(sentIdx) {
  for (let i = sentIdx + 1; i < sentences.length; i++) {
    return sentences[i].startTime;
  }
  return sentences[sentIdx].endTime;
}

// === RULE: Silence detection (>0.8s) ===
for (const gap of gaps) {
  const duration = gap.end - gap.start;
  if (duration <= SILENCE_THRESHOLD) continue;

  let sentIdx = -1;
  for (let i = 0; i < sentences.length; i++) {
    if (deletedSentences.has(i)) continue;
    const s = sentences[i];
    if (gap.start >= s.startTime - 0.5 && gap.start <= getNextSentenceStart(i) + 0.5) {
      sentIdx = i;
    }
  }
  if (sentIdx < 0 || deletedSentences.has(sentIdx)) continue;

  const deleteStart = gap.start + SILENCE_CAP;
  const deleteEnd = gap.end;
  if (deleteEnd - deleteStart < 0.1) continue;

  edits.push({
    idx: editIdx++,
    sentenceIdx: sentIdx,
    type: 'silence',
    rule: '3-é™éŸ³æ®µå¤„ç†',
    duration: parseFloat(duration.toFixed(1)),
    deleteStart: parseFloat(gap.start.toFixed(2)),
    deleteEnd: parseFloat(gap.end.toFixed(2)),
    keepDuration: SILENCE_CAP,
    reason: `é™éŸ³${duration.toFixed(1)}ç§’ï¼Œcapåˆ°${SILENCE_CAP}ç§’`
  });
}

// === RULE 1: Exact-match stutter detection (consecutive identical words) ===
// Design: catch ALL repeats, only blanket-exempt å è¯ and numbers.
// High-freq words/phrases: catch + needsReview=true â†’ LLM decides.
for (const sent of sentences) {
  if (deletedSentences.has(sent.idx)) continue;
  const words = sent.words;

  for (let i = 0; i < words.length - 1; i++) {
    const curr = words[i].text;
    const next = words[i + 1].text;

    if (curr === next && curr.length >= 1) {
      // Count total consecutive repeats
      let endRepeat = i + 1;
      while (endRepeat + 1 < words.length && words[endRepeat + 1].text === curr) {
        endRepeat++;
      }
      const repeatCount = endRepeat - i + 1; // total occurrences
      const combined = curr + next;

      // === Blanket exemptions (never flag) ===
      if (REDUPLICATED_WORDS.has(combined)) { i = endRepeat; continue; }
      if (NUMBER_CHARS.test(curr)) { i = endRepeat; continue; }
      // ABBå è¯è±å…: å•å­—ä¸”å‰ä¸€ä¸ªè¯ä¸åŒ â†’ "ç²‰å˜Ÿå˜Ÿ" ç»“æ„
      if (repeatCount === 2 && curr.length === 1 && i > 0 && words[i - 1].text !== curr) {
        i = endRepeat; continue;
      }

      // === Determine needsReview ===
      let needsReview = false;
      let reviewHint = '';
      if (repeatCount === 2) {
        if (curr.length === 1 && MAYBE_NATURAL_REPEATS.has(curr)) {
          needsReview = true;
          reviewHint = `å•å­—é«˜é¢‘è¯"${curr}"2xï¼Œå¯èƒ½æ˜¯è‡ªç„¶å£è¯­ï¼ˆå¦‚å›åº”"å¯¹å¯¹"ï¼‰ï¼Œè¯·æ ¹æ®è¯­å¢ƒåˆ¤æ–­`;
        } else if (MAYBE_NATURAL_PHRASES.has(curr)) {
          needsReview = true;
          reviewHint = `é«˜é¢‘çŸ­è¯­"${curr}"2xï¼Œå¤§å¤šæ•°æƒ…å†µæ˜¯å¡é¡¿ï¼Œä½†å¦‚"æ€ä¹ˆæ€ä¹ˆåš"å¯èƒ½æ˜¯ä¿®è¾`;
        }
      }

      const wordStartIdx = sent.wordRange[0] + i;
      const wordEndIdx = sent.wordRange[0] + endRepeat - 1; // delete all but last

      const edit = {
        idx: editIdx++,
        sentenceIdx: sent.idx,
        type: 'stutter',
        rule: '5-å¡é¡¿è¯',
        wordRange: [wordStartIdx, wordEndIdx],
        deleteText: curr.repeat(endRepeat - i),
        keepText: curr,
        deleteStart: parseFloat(words[i].start.toFixed(2)),
        deleteEnd: parseFloat(words[endRepeat - 1].end.toFixed(2)),
        reason: `"${curr}"è¿ç»­é‡å¤${repeatCount}æ¬¡ï¼Œä¿ç•™æœ€åä¸€æ¬¡`
      };
      if (needsReview) {
        edit.needsReview = true;
        edit.reviewHint = reviewHint;
        edit.confidence = 0.7;
      }
      edits.push(edit);

      i = endRepeat;
    }
  }
}

// === RULE 2: Suffix-match stutter detection (ASRåˆ†è¯è¾¹ç•Œé—®é¢˜) ===
// e.g. "åœ¨è¿™ä¸ª" + "è¿™ä¸ª" â†’ åç¼€ "è¿™ä¸ª" é‡å¤
// e.g. "ä¹Ÿå¼€å§‹" + "å¼€å§‹" â†’ åç¼€ "å¼€å§‹" é‡å¤
for (const sent of sentences) {
  if (deletedSentences.has(sent.idx)) continue;
  const words = sent.words;

  for (let i = 0; i < words.length - 1; i++) {
    const w1 = words[i].text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›""''ï¼ˆï¼‰\s]/g, '');
    const w2 = words[i + 1].text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›""''ï¼ˆï¼‰\s]/g, '');
    if (!w1 || !w2) continue;
    if (w1 === w2) continue; // already handled by exact match
    if (w1.length <= w2.length) continue; // w1 must be longer

    // Skip English words (avoid "OPEN"+"EN", "THIS"+"IS")
    if (ENGLISH_WORD.test(w1) || ENGLISH_WORD.test(w2)) continue;

    // Check if w1 ends with w2 and w2 is â‰¥2 chars
    if (w1.endsWith(w2) && w2.length >= 2) {
      const globalIdx = sent.wordRange[0] + i + 1;
      // Check no existing edit overlaps this word
      const alreadyCovered = edits.some(e =>
        e.sentenceIdx === sent.idx &&
        Math.max(words[i + 1].start, e.deleteStart || 0) < Math.min(words[i + 1].end, e.deleteEnd || 0)
      );
      if (alreadyCovered) continue;

      edits.push({
        idx: editIdx++,
        sentenceIdx: sent.idx,
        type: 'stutter',
        rule: '5-å¡é¡¿è¯(åç¼€åŒ¹é…)',
        wordRange: [globalIdx, globalIdx],
        deleteText: w2,
        keepText: w2,
        deleteStart: parseFloat(words[i + 1].start.toFixed(2)),
        deleteEnd: parseFloat(words[i + 1].end.toFixed(2)),
        reason: `åç¼€åŒ¹é…ï¼š"${w1}"æœ«å°¾ä¸"${w2}"é‡å¤`,
        needsReview: true,
        reviewHint: `ASRåˆ†è¯è¾¹ç•Œé—®é¢˜ï¼šç¬¬ä¸€ä¸ªè¯"${w1}"æœ«å°¾å·²å«"${w2}"ï¼Œç¬¬äºŒä¸ªè¯"${w2}"æ˜¯é‡å¤`,
        confidence: 0.8
      });
    }
  }
}

// === RULE: Restart marker detection (A + é‡å¯ä¿¡å· + A) ===
// Pattern: speaker says something, then "ç­‰ä¸€ä¸‹"/"é‡æ¥" etc., then repeats.
// Delete the first occurrence + restart marker.
const RESTART_MARKERS = new Set([
  'ç­‰ä¸€ä¸‹', 'é‡æ¥', 'å†è¯´ä¸€é', 'å†æ¥', 'é‡æ–°è¯´', 'é‡æ–°æ¥',
  'ç­‰ç­‰', 'ä¸å¯¹', 'è¯´é”™äº†', 'æˆ‘é‡è¯´', 'å†æ¥ä¸€é',
]);

for (const sent of sentences) {
  if (deletedSentences.has(sent.idx)) continue;
  const words = sent.words;
  if (words.length < 4) continue; // need at least: A marker A

  for (let m = 1; m < words.length - 1; m++) {
    // Check single-word and two-word markers
    let markerLen = 0;
    const w1 = words[m].text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€]/g, '');
    const w2 = m + 1 < words.length ? (w1 + words[m + 1].text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€]/g, '')) : '';

    if (RESTART_MARKERS.has(w2) && m + 1 < words.length - 1) {
      markerLen = 2;
    } else if (RESTART_MARKERS.has(w1)) {
      markerLen = 1;
    }
    if (markerLen === 0) continue;

    // Found a restart marker at position m (length markerLen)
    // Compare text before marker vs text after marker
    const beforeStart = 0;
    const beforeEnd = m; // exclusive
    const afterStart = m + markerLen;

    if (afterStart >= words.length) continue;

    // Get text snippets (first N words before and after marker)
    const compareLen = Math.min(beforeEnd - beforeStart, words.length - afterStart, 5);
    if (compareLen < 1) continue;

    const beforeText = words.slice(beforeEnd - compareLen, beforeEnd)
      .map(w => w.text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€]/g, '')).join('');
    const afterText = words.slice(afterStart, afterStart + compareLen)
      .map(w => w.text.replace(/[ï¼Œã€‚ï¼ï¼Ÿã€]/g, '')).join('');

    // Check similarity: at least 60% character overlap
    const overlap = [...beforeText].filter(c => afterText.includes(c)).length;
    const similarity = overlap / Math.max(beforeText.length, 1);

    if (similarity >= 0.6) {
      const deleteWords = words.slice(beforeStart, afterStart);
      const deleteText = deleteWords.map(w => w.text).join('');
      const keepWords = words.slice(afterStart);
      const keepText = keepWords.map(w => w.text).join('');

      // Check for duplicate with existing stutter edits
      const dupExists = edits.some(e =>
        e.sentenceIdx === sent.idx &&
        Math.abs(e.deleteStart - deleteWords[0].start) < 0.1
      );
      if (dupExists) continue;

      edits.push({
        idx: editIdx++,
        sentenceIdx: sent.idx,
        type: 'self_correction',
        rule: '8-é‡è¯´çº æ­£(restart-marker)',
        wordRange: [sent.wordRange[0] + beforeStart, sent.wordRange[0] + afterStart - 1],
        deleteText,
        keepText,
        deleteStart: parseFloat(deleteWords[0].start.toFixed(2)),
        deleteEnd: parseFloat(deleteWords[deleteWords.length - 1].end.toFixed(2)),
        reason: `é‡å¯ä¿¡å·"${words.slice(m, m + markerLen).map(w => w.text).join('')}"å‰åæ–‡æœ¬ç›¸ä¼¼(${(similarity * 100).toFixed(0)}%)ï¼Œåˆ ç¬¬ä¸€é+ä¿¡å·è¯`
      });
      break; // one restart per sentence
    }
  }
}

// Sort edits by time
edits.sort((a, b) => a.deleteStart - b.deleteStart);
edits.forEach((e, i) => e.idx = i);

// Summary
const byType = {};
let needsReviewCount = 0;
for (const e of edits) {
  byType[e.type] = (byType[e.type] || 0) + 1;
  if (e.needsReview) needsReviewCount++;
}

const totalTimeSaved = edits.reduce((sum, e) => {
  if (e.type === 'silence') {
    return sum + (e.duration - e.keepDuration);
  }
  return sum + (e.deleteEnd - e.deleteStart);
}, 0);

const result = {
  edits,
  summary: {
    totalEdits: edits.length,
    needsReview: needsReviewCount,
    byType,
    estimatedTimeSaved: `${Math.floor(totalTimeSaved / 60)}:${String(Math.floor(totalTimeSaved % 60)).padStart(2, '0')}`
  }
};

fs.writeFileSync(outputPath, JSON.stringify(result, null, 2));
console.log(`âœ… Rules layer complete: ${outputPath}`);
console.log(`   Total edits: ${edits.length} (${needsReviewCount} needsReview â†’ LLM decides)`);
console.log(`   By type:`, JSON.stringify(byType));
console.log(`   Estimated time saved: ${result.summary.estimatedTimeSaved}`);

// Show needsReview items for visibility
if (needsReviewCount > 0) {
  console.log(`\n   ğŸ” needsReview items (LLM will decide):`);
  edits.filter(e => e.needsReview).forEach(e => {
    console.log(`      S${e.sentenceIdx}: "${e.deleteText}" â€” ${e.reviewHint}`);
  });
}
