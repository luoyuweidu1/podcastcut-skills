# 步骤 5：内容分析指南

> 如何识别播客中应该删除的大块内容

## 核心原则

1. **先删大块，再调细节** — 段落级删除效率远高于逐句微调
2. **确定性优先** — 先标记100%确定要删的（技术调试、隐私），再处理需判断的（闲聊、信息密度）
3. **尊重说话人意愿** — 说话人明确说"别剪进去"的，无条件删除
4. **保留简版** — 同一段故事讲了两遍时，保留精简版，删除冗长版

---

## 两级分析流程

### Level 1：段落级扫描

**目标**：通读全文，划分话题段落，标记大块删除区间

**操作步骤**：
1. 读取 `sentences.txt` 全文
2. 按话题转换划分段落（正式开场、自我介绍、话题A、话题B…）
3. 逐段判断是否属于 6 种删除类型
4. 记录每个删除块的句子范围

### Level 2：边界微调

**目标**：确认每个删除块的精确切点

**操作步骤**：
1. 检查删除块的前 2-3 句和后 2-3 句
2. 确保切点不在对话中间（一问一答不要切开）
3. 确保切点前后的内容能自然衔接

---

## 6 种删除类型

### 1. 录前准备 (`pre_show`)

**定义**：正式开场白（"大家好，欢迎来到…"）之前的一切内容

**识别特征**：
- "能听见吗"、"我把 doc 打开"、"谁先开始"
- 讨论录制软件、音频设置
- 内容协调（"你觉得这样讲行不行"）

**处理方式**：全删，无例外

**边界判断**：找到正式开场白那句话，它之前的全部删除

---

### 2. 技术调试 (`tech_debug`)

**定义**：录制过程中出现的设备和技术问题讨论

**识别特征**：
- "耳机没电了"、"麦克风好像有问题"、"录上了吗"
- "我这边显示…"、"你那边看到…"
- Riverside、录制软件相关讨论

**处理方式**：全删

**边界判断**：
- 起点：从第一句提到技术问题的话开始
- 终点：到"好，我们继续"或正式恢复话题的那句

---

### 3. 跑题闲聊 (`chit_chat`)

**定义**：与本期播客主题无关的对话

**识别特征**：
- 在等技术修复期间的寒暄
- "你在哪个城市"、"你是哪年毕业的"、"那边房价怎么样"
- 引用其他节目、推荐不相关资源
- 对话明显脱离了当前讨论主线

**处理方式**：删除，但需注意以下例外：
- 如果闲聊中自然过渡到了主题相关内容，从过渡点开始保留
- 嘉宾的简短背景介绍即使在闲聊段中，也可能在正式介绍中重复出现（优先保留正式版）

**边界判断**：
- 注意区分"相关的轻松讨论"和"完全跑题"
- 当话题从跑题自然回归主线时，从回归点开始保留

---

### 4. 隐私信息 (`privacy`)

**定义**：说话人明确要求删除的内容，或涉及敏感个人信息

**识别特征**：
- **显式请求**："这里别剪进去了"、"这段不确定要不要"、"怕同事看到"
- **敏感内容**：具体公司内部流程、同事姓名、薪资数字、移民身份细节、离职计划

**处理方式**：无条件删除。宁可多删，不可遗漏

**边界判断**：
- 显式请求：删除被要求删除的内容段，以及请求本身（"这段别剪了"这句话也要删）
- 敏感内容：如果上下文讨论了"要不要剪这段"，整个讨论连同内容一起删

---

### 5. 重复内容 (`repeated_content`)

**定义**：同一段经历/故事被讲了两遍

**识别特征**：
- 说话人因为隐私顾虑重新讲了一个简略版
- "我重新说一遍"、"我说一个简略版的"
- 相同的故事线、相同的关键词，但详略不同

**处理方式**：
- 保留简版（通常是第二遍，更精炼）
- 删除详版（通常是第一遍，未经过滤的原始版本）
- 如果第一遍有独特信息是第二遍没有的，标记给用户决定

**边界判断**：
- 详版起点：从开始讲故事的第一句开始
- 详版终点：到隐私讨论开始之前（隐私讨论本身也要删）
- 简版的起止不动

---

### 6. 制作讨论 (`production_talk`)

**定义**：录制过程中关于剪辑策略、录制协调的内部讨论

**识别特征**：
- "这段要剪进去吗"、"我们回头可以剪掉"
- "我们要准备收尾吗"、"上一个问题是什么来着"
- 讨论节目结构、时间分配
- "那段要不要保留"、"这些 personal 的东西都不剪了"
- **录制协调**（易遗漏！）："我们补一下自我介绍哈"、"你先说还是我先说"、"要不要换个顺序"

**处理方式**：全删

**边界判断**：
- 从制作讨论开始删
- 到正式恢复播客内容时停止（如"好，那我们来聊一下…"）

**特别注意：孤立的录制协调句**

录制协调不一定成块出现，可能是夹在正式内容中的单独一句话。Level 1 扫描时容易遗漏。

识别方法：逐句扫描时，即使前后都是正式内容，如果某一句是对同伴说的录制指令（"我们补一下XX"、"你来说一下XX"），也应标记为 `production_talk`。

```
案例：
  句37: [正式内容] "...burnout 这个话题很重要。"
  句38: [录制协调] "嗯，我们补一下自我介绍哈。"  ← 单独一句也要删！
  句39: [正式内容] "好，大家好，我是..."
```

---

## 时长计算与缺口处理

### 计算公式

```
需删减 = 总时长 - 目标时长
确定性删除 = 6种类型的总时长
缺口 = 需删减 - 确定性删除
```

### 如果确定性删除已足够

直接生成 `semantic_deep_analysis.json`，不需要额外删减。

### 如果仍有缺口

按优先级逐步扩大删减范围：

1. **啰嗦重复的观点** — 同一个观点被不同说话人从不同角度说了多次，保留最精彩的一次
2. **过度详细的过渡** — "我们来聊下一个话题"、"嗯嗯嗯对对对"等无信息量的过渡
3. **信息密度低的段落** — 大段的铺垫但核心信息只有一两句话
4. **与听众弱相关的细节** — 过于个人化的细节（除非是播客的卖点）

---

## 输出格式

### semantic_deep_analysis.json

```json
{
  "version": "5.0",
  "analysisType": "two_level",
  "totalDuration": "2:08:06 (128min)",
  "targetDuration": "90min",
  "blocks": [
    {
      "id": 1,
      "range": [0, 19],
      "type": "pre_show",
      "reason": "录前准备：测噪音、谁先开口、打开文档",
      "duration": "1:06"
    }
  ],
  "sentences": [
    {
      "sentenceIdx": 0,
      "speaker": "响歌歌",
      "action": "delete",
      "blockId": 1,
      "type": "pre_show",
      "reason": "录前准备：测噪音、谁先开口、打开文档"
    },
    {
      "sentenceIdx": 20,
      "speaker": "麦雅",
      "action": "keep"
    }
  ],
  "summary": {
    "totalSentences": 1404,
    "deleteSentences": 227,
    "keepSentences": 1177,
    "deleteBlocks": 13,
    "totalDeleteDuration": "14:23",
    "deleteRatio": "16.2%"
  }
}
```

**设计说明**：
- `blocks` 给人看 — 审核时快速了解删了什么大块
- `sentences` 给脚本用 — 下游 `generate_default_selection.js` 直接消费
- 保留句只需 `sentenceIdx` + `speaker` + `action: "keep"`，不需要冗余字段
- 删除句带 `blockId`、`type`、`reason`，方便溯源

---

## 实战案例

### 案例：burnout 主题播客（2小时 → 目标90分钟）

**总时长**：128分钟
**目标时长**：90分钟
**需删减**：38分钟

**确定性删除（14分钟）**：

| 类型 | 块数 | 时长 | 典型内容 |
|------|------|------|----------|
| 录前准备 | 1 | 1:06 | 测噪音、开 doc、分工 |
| 技术调试 | 3 | 1:53 | 耳机断连、麦克风闷 |
| 跑题闲聊 | 2 | 3:22 | 杜克 vs RTP、推荐咨询师 |
| 隐私信息 | 3 | 1:54 | "别剪进去了"、移民细节 |
| 重复内容 | 2 | 4:48 | burnout背景详版、咨询师故事详版 |
| 制作讨论 | 2 | 1:20 | "这段要剪吗"、讨论收尾 |
| **合计** | **13** | **14:23** | |

**缺口**：约 24 分钟，需后续通过信息密度分析补充
